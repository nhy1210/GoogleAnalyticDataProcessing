{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExtractData_FromRawSrc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Ss5MA-heFWEB8d_piT08SRMggCSvZdgW",
      "authorship_tag": "ABX9TyM8jvghNqtKjWbLYETO42A9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhy1210/GoogleAnalyticDataProcessing/blob/main/ExtractData_FromRawSrc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYshAYivmoay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f7f50e-80ae-4039-d2ad-4eac00b1c612"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6NFfGWhsWHj"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtsSjY6tmqD2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2109d2-9bb9-43af-d410-ef59883ca975"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import ast \n",
        "import pickle \n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime, date\n",
        "from apiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "\n",
        "def extractData(individualID):\n",
        "\n",
        "  ###########################################################\n",
        "  # Get raw data from csv file\n",
        "  filename = \"Accenture_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/GA_RawData/Accenture/1_Jan2019_Sep2020/Part1/\" + filename\n",
        "  df_userID = pd.read_csv(filepath, dtype=str)\n",
        "\n",
        "  #Loading session data from pandas frame df_userID\n",
        "  session = df_userID[\"sessions\"]\n",
        "\n",
        "  ###########################################################\n",
        "  #Declare a pandas frame for adding data\n",
        "  pd_pathinfo = pd.DataFrame(columns=['UserID','sessionID','Act_datetime','Duration_Time','App_Time','Path', 'Title', 'Campaign']) # remove , 'Act_Time'\n",
        "  #pd_userBehav = pd.DataFrame(columns=['UserID', 'Total_Act','Total_Time', 'Com_Act', 'Com_Time' ,'My_Act', 'My_Time', 'Reading_Act', 'Reading_Time','Off_Hour_Cnt', 'Off_Hour_Time','OutOf_Hour_Cnt', 'OutOf_Hour_Time'])\n",
        "  pd_userBehav = pd.DataFrame(columns=['UserID', 'Total_Act', 'Com_Act', 'My_Act', 'Reading_Act','Other_Act', 'Off_Hour_Cnt', 'OutOf_Hour_Cnt', \n",
        "  'Total_Time','Com_Time' ,'My_Time', 'Reading_Time','Other_Time','Off_Hour_Time', 'OutOf_Hour_Time'])\n",
        "\n",
        "  #12/11/2020\n",
        "  pd_user = pd.DataFrame(columns=['Session', 'Avg_session_duration', \n",
        "                                  'Page_view', 'Avg_page_view', \n",
        "                                  'Avg_reading_page_view','Avg_learning_page_view_%', \n",
        "                                  'Avg_learning_duration', 'Avg_learning_duration_%', \n",
        "                                  'Duplicate_learning','No_of_unique_page_view'])\n",
        "\n",
        "  ###########################################################\n",
        "  #Iterate all the Session\n",
        "  for iSeS in range (len(session)): \n",
        "    i_Session = session[iSeS]\n",
        "\n",
        "    #Convert string to dictionary type \n",
        "    dict_session = ast.literal_eval(i_Session)\n",
        "\n",
        "    '''\n",
        "    #Example of dict_session\n",
        "    {'sessionId': '1582167399', 'deviceCategory': 'desktop', 'platform': 'Windows', 'dataSource': 'web',\n",
        "      'activities': [{'activityTime': '2020-02-20T13:56:39.796152Z', 'source': '(direct)', 'medium': '(none)', 'channelGrouping': 'Direct', 'campaign': '(not set)', 'keyword': '(not set)', 'hostname': 'accenture.myspringday.com.au', 'landingPagePath': '/articles/sleep-and-money', 'activityType': 'PAGEVIEW', \n",
        "      'pageview': {'pagePath': '/articles/sleep-and-money', 'pageTitle': 'Sleep and Money'}}], \n",
        "      'sessionDate': '2020-02-20'\n",
        "    }\n",
        "    '''\n",
        "    #========================================================\n",
        "    ses_id = dict_session[\"sessionId\"]\n",
        "    ind_activity= dict_session[\"activities\"]\n",
        "\n",
        "    #========================================================\n",
        "    #Pre-condition time \n",
        "    campaign = ind_activity[0][\"campaign\"]\n",
        "    #print(campaign)\n",
        "    act_atime = ind_activity[0][\"activityTime\"]\n",
        "    act_time = act_atime[11:-2]\n",
        "    act_hm = act_time[:5]\n",
        "    print (\"Act_time : \", act_hm)\n",
        "    time_prev = datetime.strptime(act_time, '%H:%M:%S.%f').time()\n",
        "    time_end = time_prev \n",
        "\n",
        "    #========================================================\n",
        "    for act in range (len(ind_activity)):\n",
        "      act_atime = ind_activity[act][\"activityTime\"]\n",
        "      act_date = act_atime[:10]\n",
        "      act_time = act_atime[11:-2]\n",
        "\n",
        "      #exchange time string to datatime type\n",
        "      time_cur = datetime.strptime(act_time, '%H:%M:%S.%f').time()\n",
        "      time_diff = datetime.combine(date.today(), time_prev) - datetime.combine(date.today(), time_cur)\n",
        "      time_prev = time_cur\n",
        "      if act == len(ind_activity) -1 : \n",
        "        time_end = time_cur\n",
        "      \n",
        "      time_diff_sec = time_diff.seconds\n",
        "      #print(\"Time diff : \", time_diff_sec, type(time_diff_sec))\n",
        "      #print(type(time_cur))\n",
        "      #print(\"Current_Time : \", time_cur)\n",
        "      #print(\"Prev_Time.   : \", time_prev)\n",
        "\n",
        "      '''\n",
        "      #calculate session time:\n",
        "      time_ses_diff = datetime.combine(date.today(), time_end) - datetime.combine(date.today(), time_start)\n",
        "      time_ses_dur =time_ses_diff.seconds \n",
        "      print() \n",
        "      '''\n",
        "      app_time = int(act_time[:2])\n",
        "      page_view = ind_activity[act][\"pageview\"]\n",
        "\n",
        "      path = page_view[\"pagePath\"]\n",
        "      view = page_view[\"pageTitle\"]\n",
        "      #row = [individualID, ses_id, act_date,act_time,time_diff_sec, app_time, path, view,campaign] Separate date and time => error in tableau\n",
        "      row = [individualID, ses_id, act_atime, time_diff_sec, app_time, path, view,campaign]\n",
        "      pd_pathinfo.loc[len(pd_pathinfo)] = row\n",
        "  \n",
        "  '''\n",
        "  #convert time string to datetime\n",
        "  pd_pathinfo['Act_Time'] = pd.to_datetime(pd_pathinfo['Act_Time'])\n",
        "  x = pd_pathinfo['Act_Time'][1] - pd_pathinfo['Act_Time'][2]\n",
        "  print(\"Time diff: \", x)\n",
        "  print(type(pd_pathinfo['Act_Time'][0]))\n",
        "  '''\n",
        "\n",
        "\n",
        "\n",
        "  ###########################################################\n",
        "  #Add initial filter to pandas frame (#filter_keywords = ['Home','(not set)','Not found - Error 404'])\n",
        "  filter_keywords = ['Not found - Error 404']\n",
        "  pd_pathinfo_filter = pd_pathinfo[~pd_pathinfo.Title.isin(filter_keywords)]\n",
        "  #print(\"Size of general information after filtering\", pd_pathinfo_filter.shape)\n",
        "\n",
        "  filename = \"A_AllInfo_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/1.Testing_CourseINFO/\" + filename\n",
        "  pd_pathinfo_filter.to_csv(filepath)\n",
        "\n",
        "\n",
        "  '''\n",
        "  ###########################################################\n",
        "  #Health Wellbeing Check\n",
        "  #20200930 pntn\n",
        "  pd_wct_filter = pd_pathinfo[pd_pathinfo['Title'].str.contains('Take the Wellbeing Check Tool', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_WCT_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/WCT/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_wct_filter.index) > 0: \n",
        "    pd_wct_filter.to_csv(filepath)\n",
        "  #20200110 total_act = len(pd_pathinfo.index)\n",
        "  #20200110 sum_total = pd_pathinfo['Duration_Time'].sum()\n",
        "\n",
        "  ###########################################################\n",
        "  #Better Together\n",
        "  pd_better = pd_pathinfo[pd_pathinfo['Title'].str.contains('Better Together', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_BT_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Better Together/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_better.index) > 0: \n",
        "    pd_better.to_csv(filepath)\n",
        "\n",
        "\n",
        "  ###########################################################\n",
        "  #Keeping Connected\n",
        "  pd_Kconnect = pd_pathinfo[pd_pathinfo['Title'].str.contains('Keeping Connected', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_KC_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Keeping Connected/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_Kconnect.index) > 0: \n",
        "    pd_Kconnect.to_csv(filepath)\n",
        "\n",
        "  ###########################################################\n",
        "  #Link Your Heart\n",
        "  pd_LuH = pd_pathinfo[pd_pathinfo['Title'].str.contains('Link Your Heart', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_LuH_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Link Your Heart/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_LuH.index) > 0: \n",
        "    pd_LuH.to_csv(filepath)\n",
        "\n",
        "  ###########################################################\n",
        "  #Money Habits\n",
        "  pd_MH = pd_pathinfo[pd_pathinfo['Title'].str.contains('Money Habits', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_MH_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Money Habits/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_MH.index) > 0: \n",
        "    pd_MH.to_csv(filepath)\n",
        "\n",
        "  ###########################################################\n",
        "  #Step Up In September\n",
        "  pd_StUp = pd_pathinfo[pd_pathinfo['Title'].str.contains('Step Up In September', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_StUp_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Step Up In September/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_StUp.index) > 0: \n",
        "    pd_StUp.to_csv(filepath)\n",
        "\n",
        "\n",
        "  ###########################################################\n",
        "  #Sustainable Behaviour Change\n",
        "  pd_Sus = pd_pathinfo[pd_pathinfo['Title'].str.contains('Sustainable Behaviour Change', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_Sus_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Others/Sustainable Behaviour Change/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_Sus.index) > 0: \n",
        "    pd_Sus.to_csv(filepath)\n",
        "\n",
        "  ###########################################################\n",
        "  #Get ahead of your mental fitness\n",
        "  pd_Men = pd_pathinfo[pd_pathinfo['Title'].str.contains('Get ahead of your mental fitness', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_Men_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Others/Get ahead of your mental fitness/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_Men.index) > 0: \n",
        "    pd_Men.to_csv(filepath)\n",
        "\n",
        "\n",
        "  ###########################################################\n",
        "  #New Year, New You\n",
        "  pd_NY = pd_pathinfo[pd_pathinfo['Title'].str.contains('New Year, New You', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_NY_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Others/New Year_New You/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_NY.index) > 0: \n",
        "    pd_NY.to_csv(filepath)\n",
        "\n",
        "  ###########################################################\n",
        "  #got your back this May\n",
        "  pd_May = pd_pathinfo[pd_pathinfo['Title'].str.contains('got your back this May', na=False)]\n",
        "  #Save General User information to file #Have to consider again to calculate the duration in session\n",
        "  filename = \"A_May_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/DataCollection_FromRAW/Accenture/Jan19_Sep20_Part2/Campaign/Others/got your back this May/\" + filename\n",
        "  #20200930 pntn pd_pathinfo_filter.to_csv(filepath)\n",
        "  if len(pd_May.index) > 0: \n",
        "    pd_May.to_csv(filepath)\n",
        "\n",
        "  \n",
        "  ###########################################################\n",
        "  #Reading category  \n",
        "  pd_reading_filter = pd_pathinfo[pd_pathinfo['Path'].str.contains('/articles/')]\n",
        "  filename = \"A_Rd_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/Extract_data_FromRAW/Accenture/Sep19_Feb20/Reading/\" + filename\n",
        "  pd_reading_filter.to_csv(filepath)\n",
        "  read_act = len(pd_reading_filter.index)\n",
        "  sum_read = pd_reading_filter['Duration_Time'].sum()\n",
        "  #print(\"My reading: \",read_act )\n",
        "\n",
        "  ###########################################################\n",
        "  #Community category\n",
        "  pd_com_filter = pd_pathinfo[pd_pathinfo['Path'].str.contains(\"/share-|/gallery\", na=False)]\n",
        "  filename = \"A_Com_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/Extract_data_FromRAW/Accenture/Sep19_Feb20/Community/\" + filename\n",
        "  pd_com_filter.to_csv(filepath)\n",
        "  com_act = len(pd_com_filter.index)\n",
        "  sum_com = pd_com_filter['Duration_Time'].sum()\n",
        "  #print(\"My community action: \",com_act )\n",
        "\n",
        "\n",
        "  ###########################################################\n",
        "  #My activity category\n",
        "  pd_my_filter = pd_pathinfo[pd_pathinfo['Path'].str.contains('/my/')]\n",
        "  filename = \"A_Act_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/Extract_data_FromRAW/Accenture/Sep19_Feb20/My_Activity/\" + filename\n",
        "  pd_my_filter.to_csv(filepath)\n",
        "  my_act = len(pd_my_filter.index)\n",
        "  sum_myact = pd_my_filter['Duration_Time'].sum()\n",
        "\n",
        "\n",
        "  ###########################################################\n",
        "  #Office hours _ Access time //to find user behaviour\n",
        "\n",
        "  pd_office_hour =  pd_pathinfo[(pd_pathinfo['App_Time'] >= 9) & (pd_pathinfo['App_Time'] <= 17)]\n",
        "  #pd_office_hour.sort_values(by=['App_Time'], inplace=True)\n",
        "\n",
        "  filename = \"A_OH_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/Extract_data_FromRAW/Accenture/Sep19_Feb20/Office_Hours/\" + filename\n",
        "  pd_office_hour.to_csv(filepath)\n",
        "  OH_cnt = len(pd_office_hour.index)\n",
        "  sum_office_hour = pd_office_hour['Duration_Time'].sum()\n",
        "\n",
        "  ###########################################################\n",
        "  #Out of Office hours _ Access time //to find user behaviour\n",
        "  outofOH_cnt = total_act - OH_cnt\n",
        "\n",
        "  ###########################################################\n",
        "  #pd_userBehavior dataframe\n",
        "  ['UserID', 'Total_Act', 'Com_Act', 'My_Act', 'Reading_Act','Other_Act', \n",
        "  'Off_Hour_Cnt', 'OutOf_Hour_Cnt', \n",
        "  'Total_Time','Com_Time' ,'My_Time', 'Reading_Time','Other_Time',\n",
        "  'Off_Hour_Time', 'OutOf_Hour_Time'])\n",
        "  ###########################################################\n",
        "\n",
        "  othe_act = total_act - (com_act + my_act + read_act)\n",
        "  sum_other = sum_total - (sum_com  + sum_myact + sum_read)\n",
        "  final_row = [individualID, total_act,  com_act, my_act, read_act , othe_act, \n",
        "               OH_cnt,  outofOH_cnt,\n",
        "               sum_total, sum_com,sum_myact,sum_read, sum_other,\n",
        "               sum_office_hour,sum_total - sum_office_hour]\n",
        "  pd_userBehav.loc[len(pd_userBehav)]  = final_row\n",
        "  #Save into file \n",
        "  filename = \"UserBehav_\"+ individualID + \".csv\"\n",
        "  filepath = \"/content/drive/My Drive/Extract_data_FromRAW/Accenture/Sep19_Feb20/FinalCount/\" + filename\n",
        "  pd_userBehav.to_csv(filepath)\n",
        "  '''\n",
        "'''\n",
        "SCOPES = ['https://www.googleapis.com/auth/analytics.readonly']\n",
        "KEY_FILE_LOCATION = '/content/drive/My Drive/PythonGA/client_secrets.json'\n",
        "#VIEW_ID = '180260411' #Success Apps\n",
        "VIEW_ID = '179984294'\n",
        "'''\n",
        "def userId_data():\n",
        "  #prepare UserID - loading from csv file to pandas frame\n",
        "  df_userID = pd.read_csv('/content/drive/My Drive/PythonGA/User Explorer 20190101-20200930_P1.csv', dtype=str)\n",
        "  #print(df_userID['Client Id'])\n",
        "  return df_userID\n",
        "'''\n",
        "def initialize_analyticsreporting():\n",
        "  \"\"\"Initializes an Analytics Reporting API V4 service object.\n",
        "\n",
        "  Returns:\n",
        "    An authorized Analytics Reporting API V4 service object.\n",
        "  \"\"\"\n",
        "  credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
        "      KEY_FILE_LOCATION, SCOPES)\n",
        "\n",
        "  # Build the service object.\n",
        "  analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
        "\n",
        "  return analytics\n",
        "\n",
        "\n",
        "def get_report(analytics, IndUserID):\n",
        "  \"\"\"Queries the Analytics Reporting API V4.\n",
        "\n",
        "  Args:\n",
        "    analytics: An authorized Analytics Reporting API V4 service object.\n",
        "  Returns:\n",
        "    The Analytics Reporting API V4 response.\n",
        "  \"\"\"\n",
        "  return analytics.userActivity().search(\n",
        "        body = {\n",
        "            \"viewId\": VIEW_ID,\n",
        "            #\"viewId\" : \"8925304082524788925\",\n",
        "            \"user\": {\n",
        "                \"type\": \"CLIENT_ID\",\n",
        "                #\"userId\": \"1227477603.1582680286\"\n",
        "                \"userId\": IndUserID\n",
        "            },\n",
        "            \"dateRange\": {\n",
        "                \"startDate\": \"2019-09-01\",\n",
        "                \"endDate\": \"2020-02-29\",\n",
        "            }\n",
        "        },\n",
        "        quotaUser='my-user-1'\n",
        "    ).execute()\n",
        "\n",
        "'''  \n",
        "\n",
        "def main():\n",
        "\n",
        "  #==========================================================\n",
        "  #Loading UserID into pandas frame\n",
        "  df_user = userId_data() \n",
        "  '''\n",
        "  #==========================================================\n",
        "  #Loading all UserIDS info from raw data to extract userful information\n",
        "  for ind in df_user.index:\n",
        "    clientID = df_user['Client Id'][ind]\n",
        "    extractData(clientID)\n",
        "    print(\"ClienID : \", clientID)\n",
        "\n",
        "  '''\n",
        "  #==========================================================\n",
        "  # Loading one UserID info from raw data to extract userful information\n",
        "  clientID = df_user['Client Id'][0]\n",
        "  #clientID =\"5422991.1569975384\"\n",
        "  #clientID = \"1009165876.1580704555\"\n",
        "  extractData(clientID)\n",
        "  print(\"ClienID : \", clientID)\n",
        "  #==========================================================\n",
        "  \n",
        "  \n",
        "  \n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Act_time :  00:53\n",
            "Act_time :  22:16\n",
            "ClienID :  1684510276.1546341413\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}